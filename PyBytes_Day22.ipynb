{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiQQF1+0jF+wKCMNeY9SLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthi-rajendran-DS/Medium-Implementations/blob/main/PyBytes_Day22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resilient Distributed Dataset - CheatSheet"
      ],
      "metadata": {
        "id": "cwKW_GpXU5pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD stands for Resilient Distributed Dataset. It is one of the core data structures in Apache Spark, designed to handle distributed data processing tasks efficiently.\n",
        "\n",
        "An RDD is an immutable, partitioned collection of objects that can be processed in parallel across a cluster of computers. It represents a logical division of data that can be stored in memory or persisted on disk. RDDs are fault-tolerant, meaning they can recover from failures and maintain data consistency.\n",
        "\n",
        "Key characteristics of RDDs:\n",
        "\n",
        "**Distributed**: RDDs are distributed across multiple nodes in a cluster, allowing parallel processing of data. Each RDD partition is processed independently on different nodes.\n",
        "\n",
        "**Resilient**: RDDs are fault-tolerant, meaning they can recover from node failures. If a partition is lost, RDDs can recompute it using the lineage of operations.\n",
        "\n",
        "**Immutable**: RDDs are read-only and cannot be modified once created. However, you can create new RDDs by transforming existing ones using various operations.\n",
        "\n",
        "**Lazily Evaluated**: RDD transformations are lazily evaluated, which means they are not executed immediately. Transformations build a lineage of operations that are only computed when an action is called.\n",
        "\n",
        "**Cacheable**: RDDs can be cached in memory, allowing faster access for subsequent operations. Caching is beneficial for iterative algorithms or when the same RDD is used multiple times.\n",
        "\n",
        "RDDs provide a programming abstraction that allows developers to write parallel and fault-tolerant data processing tasks without worrying about the underlying distribution and fault recovery mechanisms. However, in recent versions of Apache Spark, the DataFrame and Dataset APIs have become the preferred interface for working with structured and semi-structured data due to their more optimized performance and ease of use.\n",
        "\n",
        "Note: Starting from Spark 3.0, RDDs are considered a low-level API, and the DataFrame and Dataset APIs are recommended for most use cases.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "84MBjAYKU6jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MixKOTkzVRhM",
        "outputId": "719ba0db-89d2-4e75-83de-463b04b8e629"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=d4f4515888a0f55de0b6192331e2839da7a21bcab71738df79d93a9724522206\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n6AKxQSpUwgf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Create an RDD from a list:\n",
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "# Apply a transformation to an RDD:\n",
        "transformed_rdd = rdd.map(lambda x: x * 2)\n",
        "# Filter elements in an RDD based on a condition:\n",
        "filtered_rdd = rdd.filter(lambda x: x > 3)\n",
        "# Reduce an RDD using a custom function:\n",
        "result = rdd.reduce(lambda x, y: x + y)\n",
        "# Collect the elements of an RDD into a list:\n",
        "collected_list = rdd.collect()\n",
        "# Count the number of elements in an RDD:\n",
        "count = rdd.count()\n",
        "# Save an RDD as a text file:\n",
        "rdd.saveAsTextFile(\"/content/sample_data/output.txt\")"
      ]
    }
  ]
}